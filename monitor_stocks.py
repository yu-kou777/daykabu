import yfinance as yf
import pandas as pd
import pandas_ta as ta
import requests
import time
import io
import os
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup

# --- è¨­å®š ---
DISCORD_WEBHOOK_URL = "https://discordapp.com/api/webhooks/1472281747000393902/Fbclh0R3R55w6ZnzhenJ24coaUPKy42abh3uPO-fRjfQulk9OwAq-Cf8cJQOe2U4SFme"

def calculate_rci(series, period):
    n = period
    rank_period = pd.Series(range(n, 0, -1))
    def rci_func(x):
        d = ((pd.Series(x).rank(ascending=False) - rank_period)**2).sum()
        return (1 - (6 * d) / (n * (n**2 - 1))) * 100
    return series.rolling(window=n).apply(rci_func)

def is_peak_down(series):
    if len(series) < 4: return False
    return (series.iloc[-2] > series.iloc[-3]) and (series.iloc[-2] > series.iloc[-1])

def is_trough_up(series):
    if len(series) < 4: return False
    return (series.iloc[-2] < series.iloc[-3]) and (series.iloc[-2] < series.iloc[-1])

def get_latest_prime_list():
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        base_url = "https://www.jpx.co.jp/markets/statistics-equities/misc/01.html"
        res = requests.get(base_url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        xls_path = ""
        for a in soup.find_all('a', href=True):
            if 'data_j.xls' in a['href']:
                xls_path = a['href']
                break
        if not xls_path: raise Exception("Excelãƒªãƒ³ã‚¯æœªæ¤œå‡º")
        full_url = "https://www.jpx.co.jp" + xls_path
        resp = requests.get(full_url, headers=headers)
        df = pd.read_excel(io.BytesIO(resp.content), dtype={'ã‚³ãƒ¼ãƒ‰': str})
        prime_df = df[df['å¸‚å ´ãƒ»å•†å“åŒºåˆ†'].str.contains('ãƒ—ãƒ©ã‚¤ãƒ ', na=False)]
        return {f"{row['ã‚³ãƒ¼ãƒ‰']}.T": row['éŠ˜æŸ„å'] for _, row in prime_df.iterrows()}
    except:
        return {"9101.T": "æ—¥æœ¬éƒµèˆ¹", "6481.T": "THK"}

if __name__ == "__main__":
    jst = timezone(timedelta(hours=9))
    now_str = datetime.now(jst).strftime('%H:%M')
    ticker_map = get_latest_prime_list()
    ticker_list = list(ticker_map.keys())
    
    requests.post(DISCORD_WEBHOOK_URL, json={"content": f"ðŸš€ **å“¨æˆ’é–‹å§‹({len(ticker_list)}ç¤¾)** ({now_str})"})

    # --- æ”¹è‰¯ï¼šåˆ†å‰²ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---
    chunk_size = 400
    all_data = pd.DataFrame()
    for i in range(0, len(ticker_list), chunk_size):
        chunk = ticker_list[i : i + chunk_size]
        print(f"ðŸ“¦ ã‚¹ã‚­ãƒ£ãƒ³ä¸­... {i} ï½ž {i+len(chunk)}")
        data_chunk = yf.download(chunk, period="6mo", interval="1d", group_by='ticker', threads=True)
        all_data = pd.concat([all_data, data_chunk], axis=1)
        time.sleep(5) # 5ç§’ä¼‘æ†©ã—ã¦BANã‚’é˜²ã

    found_count = 0
    for ticker in ticker_list:
        try:
            df = all_data[ticker].dropna()
            if len(df) < 30: continue
            df.ta.rsi(length=14, append=True)
            df['RCI9'] = calculate_rci(df['Close'], 9)
            df['RCI26'] = calculate_rci(df['Close'], 26)
            curr, prev = df.iloc[-1], df.iloc[-2]

            peak_down = is_peak_down(df['RSI_14']) and is_peak_down(df['RCI9'])
            trough_up = is_trough_up(df['RSI_14']) and is_trough_up(df['RCI9'])
            gc = (prev['RCI9'] <= prev['RCI26']) and (curr['RCI9'] > curr['RCI26'])
            dc = (prev['RCI9'] >= prev['RCI26']) and (curr['RCI9'] < curr['RCI26'])

            signal = None
            reason = []
            if peak_down or dc:
                signal = "ðŸ”»ã€å£²ã‚Šè­¦æˆ’ã€‘"
                if peak_down: reason.append("RSI/RCIåŒæœŸå±±")
                if dc: reason.append("RCIãƒ‡ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹")
            elif trough_up or gc:
                signal = "ðŸ”¥ã€è²·ã„æ¤œè¨Žã€‘"
                if trough_up: reason.append("RSI/RCIåŒæœŸè°·")
                if gc: reason.append("RCIã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹")

            if signal:
                found_count += 1
                content = f"ðŸ¦… **{signal}**\n**{ticker_map[ticker]}({ticker})**\nâ”” ä¾¡æ ¼: {int(curr['Close'])}å†† / RSI: {round(curr['RSI_14'], 1)}\nâ”” ç†ç”±: {' / '.join(reason)}"
                requests.post(DISCORD_WEBHOOK_URL, json={"content": content})
                time.sleep(1)
        except:
            continue

    requests.post(DISCORD_WEBHOOK_URL, json={"content": f"âœ… **å“¨æˆ’å®Œäº†** åˆè‡´: {found_count}ä»¶"})

