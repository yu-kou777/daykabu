import yfinance as yf
import pandas as pd
import pandas_ta as ta
import requests
import time
import io
import re
import os
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup

# --- è¨­å®š ---
DISCORD_WEBHOOK_URL = "https://discordapp.com/api/webhooks/1472281747000393902/Fbclh0R3R55w6ZnzhenJ24coaUPKy42abh3uPO-fRjfQulk9OwAq-Cf8cJQOe2U4SFme"

def calculate_rci(series, period):
    n = period
    rank_period = pd.Series(range(n, 0, -1))
    def rci_func(x):
        d = ((pd.Series(x).rank(ascending=False) - rank_period)**2).sum()
        return (1 - (6 * d) / (n * (n**2 - 1))) * 100
    return series.rolling(window=n).apply(rci_func)

def is_peak_down(series):
    if len(series) < 4: return False
    return (series.iloc[-2] > series.iloc[-3]) and (series.iloc[-2] > series.iloc[-1])

def is_trough_up(series):
    if len(series) < 4: return False
    return (series.iloc[-2] < series.iloc[-3]) and (series.iloc[-2] < series.iloc[-1])

def get_latest_prime_list():
    """JPXã®ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®Excelãƒªãƒ³ã‚¯ã‚’æ¤œå‡ºã—ã€è‹±æ•°å­—ã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã—ã¦èª­ã¿è¾¼ã‚€"""
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        base_url = "https://www.jpx.co.jp/markets/statistics-equities/misc/01.html"
        res = requests.get(base_url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        xls_path = ""
        for a in soup.find_all('a', href=True):
            if 'data_j.xls' in a['href']:
                xls_path = a['href']
                break
        
        if not xls_path:
            raise Exception("Excelãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
        full_url = "https://www.jpx.co.jp" + xls_path
        print(f"ðŸ“¡ æœ€æ–°åç°¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {full_url}")
        
        resp = requests.get(full_url, headers=headers)
        # ã‚³ãƒ¼ãƒ‰åˆ—ã‚’æ–‡å­—åˆ—(str)ã¨ã—ã¦èª­ã¿è¾¼ã‚€è¨­å®šã‚’è¿½åŠ 
        df = pd.read_excel(io.BytesIO(resp.content), dtype={'ã‚³ãƒ¼ãƒ‰': str})
        
        # ãƒ—ãƒ©ã‚¤ãƒ å¸‚å ´ã®ã¿æŠ½å‡º
        prime_df = df[df['å¸‚å ´ãƒ»å•†å“åŒºåˆ†'].str.contains('ãƒ—ãƒ©ã‚¤ãƒ ', na=False)]
        
        # è‹±æ•°å­—ã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œï¼ˆintå¤‰æ›ã‚’å‰Šé™¤ï¼‰
        return {f"{row['ã‚³ãƒ¼ãƒ‰']}.T": row['éŠ˜æŸ„å'] for _, row in prime_df.iterrows()}
    except Exception as e:
        print(f"âŒ ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {"9101.T": "æ—¥æœ¬éƒµèˆ¹", "6481.T": "THK", "7203.T": "ãƒˆãƒ¨ã‚¿"}

if __name__ == "__main__":
    jst = timezone(timedelta(hours=9))
    now_str = datetime.now(jst).strftime('%H:%M')
    
    ticker_map = get_latest_prime_list()
    ticker_list = list(ticker_map.keys())
    
    # é–‹å§‹é€šçŸ¥ï¼ˆ16XXç¤¾ã¨è¡¨ç¤ºã•ã‚Œã‚Œã°æˆåŠŸã§ã™ï¼ï¼‰
    requests.post(DISCORD_WEBHOOK_URL, json={"content": f"ðŸš€ **ãƒ—ãƒ©ã‚¤ãƒ å¸‚å ´({len(ticker_list)}ç¤¾) é«˜ç²¾åº¦å“¨æˆ’ã‚’é–‹å§‹** ({now_str})"})

    # ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—
    all_data = yf.download(ticker_list, period="6mo", interval="1d", group_by='ticker', threads=True)

    found_count = 0
    for ticker in ticker_list:
        try:
            df = all_data[ticker].dropna()
            if len(df) < 30: continue

            df.ta.rsi(length=14, append=True)
            df['RCI9'] = calculate_rci(df['Close'], 9)
            df['RCI26'] = calculate_rci(df['Close'], 26)
            
            curr, prev = df.iloc[-1], df.iloc[-2]

            # åŒæœŸãƒ”ãƒ¼ã‚¯åˆ¤å®š
            peak_down = is_peak_down(df['RSI_14']) and is_peak_down(df['RCI9'])
            trough_up = is_trough_up(df['RSI_14']) and is_trough_up(df['RCI9'])
            
            # RCIã‚¯ãƒ­ã‚¹
            gc = (prev['RCI9'] <= prev['RCI26']) and (curr['RCI9'] > curr['RCI26'])
            dc = (prev['RCI9'] >= prev['RCI26']) and (curr['RCI9'] < curr['RCI26'])

            signal = None
            reason = []
            if peak_down or dc:
                signal = "ðŸ”»ã€å£²ã‚Šè­¦æˆ’ã€‘"
                if peak_down: reason.append("RSI/RCIåŒæœŸå±±")
                if dc: reason.append("RCIãƒ‡ãƒƒãƒ‰ã‚¯ãƒ­ã‚¹")
            elif trough_up or gc:
                signal = "ðŸ”¥ã€è²·ã„æ¤œè¨Žã€‘"
                if trough_up: reason.append("RSI/RCIåŒæœŸè°·")
                if gc: reason.append("RCIã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¯ãƒ­ã‚¹")

            if signal:
                found_count += 1
                name = ticker_map.get(ticker, "ä¸æ˜Ž")
                content = (
                    f"ðŸ¦… **{signal}**\n**{name}({ticker})**\n"
                    f"â”” ä¾¡æ ¼: {int(curr['Close'])}å†† / RSI: {round(curr['RSI_14'], 1)}\n"
                    f"â”” ç†ç”±: {' / '.join(reason)}"
                )
                requests.post(DISCORD_WEBHOOK_URL, json={"content": content})
                time.sleep(0.5)
        except:
            continue

    requests.post(DISCORD_WEBHOOK_URL, json={"content": f"âœ… **å“¨æˆ’å®Œäº†** åˆè‡´: {found_count}ä»¶"})

